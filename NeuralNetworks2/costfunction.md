# 代价函数（cost function）
## 1.代价函数概述
代价函数（有的地方也叫损失函数，Loss Function）在机器学习中的每一种算法中都很重要，因为训练模型的过程就是优化代价函数的过程，代价函数对每个参数的偏导数就是梯度下降中提到的梯度，防止过拟合时添加的正则化项也是加在代价函数后面的。
## 2.什么是代价函数
假设有训练样本(x,y)，模型为h，参数为θ。

h(θ) = θTx（θT表示θ的转置）。

（1）概况来讲，任何能够衡量模型预测出来的值h(θ)与真实值y之间的差异的函数都可以叫做代价函数C(θ)，如果有多个样本，则可以将所有代价函数的取值求均值，记做J(θ)。因此很容易就可以得出以下关于代价函数的性质：

- 对于每种算法来说，代价函数不是唯一的；
- 代价函数是参数θ的函数；
- 总的代价函数J(θ)可以用来评价模型的好坏，代价函数越小说明模型和参数越符合训练样本(x, y)；
- J(θ)是一个标量；

（2）当我们确定了模型h，后面做的所有事情就是训练模型的参数θ。那么什么时候模型的训练才能结束呢？这时候也涉及到代价函数，由于代价函数是用来衡量模型好坏的，我们的目标当然是得到最好的模型（也就是最符合训练样本(x, y)的模型）。因此训练参数的过程就是不断改变θ，从而得到更小的J(θ)的过程。

理想情况下，当我们取到代价函数J的最小值时，就得到了最优的参数θ，记为：

```math
min(J(Theta))
```
例如，J(θ) =0，表示我们的模型完美的拟合了观察的数据，没有任何误差。

（3）在优化参数θ的过程中，最常用的方法是梯度下降，这里的梯度就是代价函数J(θ)对θ1, θ2, ..., θn的偏导数。由于需要求偏导，我们可以得到另一个关于代价函数的性质：

- 选择代价函数时，最好挑选对参数θ可微的函数（全微分存在，偏导数一定存在）
## 3.代价函数的常见形式

经过上面的描述，一个好的代价函数需要满足两个最基本的要求：
- 能够评价模型的准确性
- 对参数θ可微
### 3.1 均方误差

在线性回归中，最常用的是均方误差(Mean squared error)，具体形式为：

![image](http://img.blog.csdn.net/20170224091950219)

m：训练样本的个数；

hθ(x)：用参数θ和x预测出来的y值；

y：原训练样本中的y值，也就是标准答案

下角标(i)：第i个样本

### 均方差与sigmoid函数

![image](http://i.imgur.com/TVWKd7e.jpg)


因为sigmoid函数的性质，导致sigmoid′(z)在z取大部分值时会很小（如上图标出来的两端，几近于平坦），这样会使得theta和b更新非常慢）。

### 3.2 交叉熵

在逻辑回归中，最常用的是代价函数是交叉熵(Cross Entropy)，交叉熵是一个常见的代价函数，在神经网络中也会用到。下面是《神经网络与深度学习》一书对交叉熵的解释：

> 交叉熵是对「出乎意料」（译者注：原文使用suprise）的度量。神经元的目标是去计算函数y, 且y=y(x)。但是我们让它取而代之计算函数a, 且a=a(x)。假设我们把a当作y等于1的概率，1−a是y等于0的概率。那么，交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。
 

在1948年，克劳德·艾尔伍德·香农将热力学的熵，引入到信息论，因此它又被称为香农熵(Shannon Entropy)，它是香农信息量(Shannon Information Content, SIC)的期望。

香农信息量用来度量不确定性的大小：一个事件的香农信息量等于0，表示该事件的发生不会给我们提供任何新的信息，例如确定性的事件，发生的概率是1，发生了也不会引起任何惊讶；当不可能事件发生时，香农信息量为无穷大，这表示给我们提供了无穷多的新信息，并且使我们无限的惊讶。更多解释可以看这里。
HA(x)=−[p(xA)log(p(xA))+(1−p(xA))log(1−p(xA))]=0.4690 

[交叉熵代价函数](http://blog.csdn.net/u012162613/article/details/44239919)
[交叉熵](http://blog.csdn.net/rtygbwwwerr/article/details/50778098)