# 决策树 
## 1.决策树概述
决策树利用分层的概念将一个复杂的决策问题分解为多个简单的判断问题，最后逐级得到最大支持度的决策结果。
## 2.决策树的特性
- 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
- 缺点：可能产生过度匹配问题
- 适用数据类型：数值型和标称型
## 3.决策树算法原理 
决策树概念比较简单，用一个男女相亲的例子来描述决策树原理如下： 

![image](http://img.blog.csdn.net/20160503104953737)

这里写图片描述
### 3.1决策树的构造 
在构造决策树时，我们需要解决的第一个问题就是当前数据集上哪个特征在划分数据分类时起决定性作用。 
### 3.3.1信息增益 
划分数据集的最大原则是：将无序数据变得更加有序。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学。在划分数据之前和之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。

设训练数据为D,则特征A对训练集D的信息增益g(D,A),定义为集合D的信息熵与特征A给定的条件下的经验熵之差，即： 

```math
g(D,A)=H(D)-H(D|A)
```
设训练集D共有k个类别|D|表示训练样本的个数，|C~k~|表示第k个类的个数，其中A共有n个属性，经A划分后可以将D划分为：D~1~,D~2~,...D~n~个子集每个最忌的个数为D~i~，经验熵为这些子集信息熵的加权平均，计算公式如下图所示。

![image](http://img.blog.csdn.net/20170103223027986?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdGFveWFucWk4OTMy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 3.3.2划分数据集 
循环计算所有特征划分数据集后的香农熵，并利用原始数据集的熵和划分后的熵相减得到该特征划分的信息增益。取信息增益最大的划分，作为最好的划分方式。
### 3.3.3递归构建决策树 
递归构建决策树工作原理如下：得到原始数据集，然后基于最好的特征值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。

递归结束的条件是：程序遍历完所有划分数据集的特征，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子节点或者终止块，任何到达叶子节点的数据必然属于叶子节点的分类。 

如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用多数表决的方法决定该叶子节点的分类。
### 3.3.4决策树的存储
使用python模块pickle序列化对象